# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Mistral-7B-Instruct-v0.3.Q5_K_M is fast enough on unaccelerated CPU and
    # takes 9GiB of RAM which should work on most computers. It has a permissive
    # license.
    # Use "python" to use the embedded pytorch generator.
    model: Mistral-7B-Instruct-v0.3.Q5_K_M
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""
  settings:
    # TODO: fine-tune them all.
    # Default system prompt to use. You can tell it to be funny or sarcastic.
    # Beware that if you do so, tool calling won't work as well.
    prompt_system: "You are an AI assistant. You reply with short answers."
    # Took a few examples from https://stablediffusion.fr/prompts (not
    # affiliated with Stable AI). I feel we need to split it into "here's a good
    # choice of keywords, please be creative and invent your own" and not give
    # too many specific prompt examples otherwise we steer the results too much
    # to fall into one of the prompt style.
    prompt_image: "You are autoregressive language model that specializes in creating perfect, outstanding prompts for generative art models like Stable Diffusion.
      Your job is to take user ideas, capture ALL main parts, and turn into amazing dense prompts.
      You have to capture everything from the user's prompt and then use your talent to make it amazing.
      You are a master of art styles, terminology, pop culture, and photography across the globe.
      Respond only with one prompt.
      Exclude article words.
      Be decisive.
      Examples of excellent prompts:\n
      - stardew valley, fine details, 4k resolution\n
      - 2D Vector Illustration of a child with soccer ball Art for Sublimation, Design Art, Chrome Art, Painting and Stunning Artwork, Highly Detailed Digital Painting, Airbrush Art, Highly Detailed Digital Artwork, Dramatic Artwork, stained antique yellow copper paint, digital airbrush art, detailed by Mark Brooks, Chicano airbrush art, Swagger! snake Culture\n
      - /image_manual image_prompt:well known person wearing biker (leather jacket), 4k resolution, a masterpiece\n
      - Cute small dog sitting in a movie theater eating popcorn watching a movie, unreal engine, cozy indoor lighting, artstation, detailed, digital painting, cinematic, character design by mark ryden and pixar and hayao miyazaki, unreal 5, daz, hyperrealistic, octane render\n
      - phone operator, business suit, desk, hidef, 4k\n
      - abstract, picasso, oil painting, flower, high quality\n
      - cartoon character of a person with a hoodie, in style of cytus and deemo, ork, gold chains, realistic anime cat, dripping black goo, lineage revolution style, thug life, cute anthropomorphic bunny, balrog, arknights, aliased, very buff, black and red and yellow paint, painting illustration collage style, character composition in vector with white background\n
      - seascape by Ray Collins and artgerm, front view of a perfect wave, sunny background, ultra detailed water, 4k resolution\n
      - Ethereal gardens of marble built in a shining teal river in future city, gorgeous ornate multi-tiered fountain, futuristic, intricate elegant highly detailed lifelike photorealistic realistic painting, long shot, studio lighting, octane render, by Dorian Cleavenger\n
      - smooth meat table, restaurant, paris, elegant, lights\n
      - highly detailed, majestic royal tall ship on a calm sea,realistic painting, by Charles Gregory Artstation and Antonio Jacobsen and Edward Moran, (long shot), clear blue sky, intricate details, 4k\n
      - Gandalf from Lord of the Rings, diffuse lighting, fantasy, intricate elegant highly detailed lifelike photorealistic digital painting, artstation\n
      - jelmer siekmans, diffuse lighting, fantasy, intricate elegant highly detailed lifelike photorealistic digital painting, goku"
    prompt_labels: "You are autoregressive language model that specializes in creating perfect, dense, outstanding meme text. Your job is to take user ideas, capture ALL main parts, and turn into amazing snarky meme labels. You have to capture everything from the user's prompt and then use your talent to make it amazing filled with sarcasm. Respond only with the new meme text. Make it as succinct as possible. Use few words. Use exactly one comma. Exclude article words."


# You can remove this section. The one embedded in
# https://github.com/maruel/sillybot/blob/main/default_config.yml will be used
# automatically.
knownllms:
  # Gemma 2 family:
  # https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315
  - repo: bartowski/gemma-2-9b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-9b-it-
    upstream: google/gemma-2-9b-it
    # https://ai.google.dev/gemma/docs/formatting
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<start_of_turn>user\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<start_of_turn>user\n"
    #  user_token_end: "<end_of_turn>\n"
    #  assistant_token_start: "<start_of_turn>model\n"
    #  assistant_token_end: "<end_of_turn>\n"
  - repo: bartowski/gemma-2-27b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-27b-it-
    upstream: google/gemma-2-27b-it

  # Llama 3 family:
  # https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6
  - repo: bartowski/Meta-Llama-3-8B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-8B-Instruct-
    upstream: meta-llama/Meta-Llama-3-8B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    #prompt_encoding:
    #  begin_of_text: "<|begin_of_text|>"
    #  system_token_start: "<|start_header_id|>system<|end_header_id|>\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<|start_header_id|>user<|end_header_id|>\n"
    #  user_token_end: "<|eot_id|>\n"
    #  assistant_token_start: "<|start_header_id|>assistant<|end_header_id|>\n"
    #  assistant_token_end: "<|eot_id|>\n"
    # TODO: This repo put Q5_K_L and higher into subdirectories, which is
    # unsupported.
  - repo: bartowski/Meta-Llama-3-70B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-70B-Instruct-
    upstream: meta-llama/Meta-Llama-3-70B-Instruct

  # Mistral family
  # https://huggingface.co/mistralai
  - repo: bartowski/Mistral-7B-Instruct-v0.3-GGUF
    packagingtype: gguf
    basename: Mistral-7B-Instruct-v0.3-
    upstream: mistralai/Mistral-7B-Instruct-v0.3
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    prompt_encoding:
      begin_of_text:                "<s>"
      system_token_start:           "[INST]\u2581"
      system_token_end:             " [/INST]"
      user_token_start:             "[INST]\u2581"
      user_token_end:               "[/INST]"
      assistant_token_start:        ""
      assistant_token_end:          "</s>"
      tools_available_token_start:  "[AVAILABLE_TOOLS]\u2581"
      tools_available_token_end:    "[/AVAILABLE_TOOLS]"
      tool_call_token_start:        "[TOOL_CALLS]\u2581"
      tool_call_token_end:          "</s>"
      tool_call_result_token_start: "[TOOL_RESULTS]\u2581"
      tool_call_result_token_end:   "[/TOOL_RESULTS]"
    # bartowski/Mixtral-8x22B-v0.1-GGUF only contain segmented files and I
    # didn't implement support yet.
  - repo: MaziyarPanahi/Mixtral-8x7B-Instruct-v0.1-GGUF
    packagingtype: gguf
    basename: Mixtral-8x7B-Instruct-v0.1.
    upstream: mistralai/Mixtral-8x7B-Instruct-v0.1
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    # and
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/sentencepiece.py#L348
    # See py/mistral_test.py
    #prompt_encoding:
    #  begin_of_text: "<s>"
    #  system_token_start: "[INST]▁"
    #  system_token_end: " [/INST]\n"
    #  user_token_start: "[INST]▁"
    #  user_token_end: " [/INST]\n"
    #  assistant_token_start: ""
    #  assistant_token_end: ""

  # Phi-3 family
  # https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3
  - repo: bartowski/Phi-3.1-mini-4k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3.1-mini-4k-instruct-
    upstream: microsoft/Phi-3-mini-4k-instruct
    # https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#chat-format
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<|system|>\n"
    #  system_token_end: "<|end|>\n"
    #  user_token_start: "<|user|>\n"
    #  user_token_end: "<|end|>\n"
    #  assistant_token_start: "<|assistant|>\n"
    #  assistant_token_end: "<|end|>\n"
  - repo: bartowski/Phi-3.1-mini-128k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3.1-mini-128k-instruct-
    upstream: microsoft/Phi-3-mini-128k-instruct
  # Nobody uploaded Phi-3-small-* gguf files. Weird.
  - repo: bartowski/Phi-3-medium-4k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3-medium-4k-instruct-
    upstream: microsoft/Phi-3-medium-4k-instruct
  - repo: bartowski/Phi-3-medium-128k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3-medium-128k-instruct-
    upstream: microsoft/Phi-3-medium-128k-instruct

  # Qwen 2 family
  # https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f
  - repo: Qwen/Qwen2-0.5B-Instruct-GGUF
    packagingtype: gguf
    basename: qwen2-0_5b-instruct-
    upstream: Qwen/Qwen2-0.5B-Instruct
    # https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md#special-tokens
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<|im_start|>\n"
    #  system_token_end: "<|im_end|>\n"
    #  user_token_start: "<|im_start|>\n"
    #  user_token_end: "<|im_end|>\n"
    #  assistant_token_start: ""
    #  assistant_token_end: "<|end_of_text|>\n"
  - repo: Qwen/Qwen2-1.5B-Instruct-GGUF
    packagingtype: gguf
    basename: qwen2-1_5b-instruct-
    upstream: Qwen/Qwen2-1.5B-Instruct
  - repo: Qwen/Qwen2-7B-Instruct-GGUF
    packagingtype: gguf
    basename: qwen2-7b-instruct-
    upstream: Qwen/Qwen2-7B-Instruct
