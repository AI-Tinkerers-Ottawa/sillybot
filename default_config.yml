# Copyright 2024 Marc-Antoine Ruel. All rights reserved.
# Use of this source code is governed under the Apache License, Version 2.0
# that can be found in the LICENSE file.

# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Mistral-7B-Instruct-v0.3.Q5_K_M is fast on unaccelerated CPU and takes
    # 9GiB of RAM which should work on most computers. It has a permissive
    # license.
    # Use "python" to use the embedded pytorch generator.
    model:        "Mistral-7B-Instruct-v0.3.Q5_K_M"
    # Default system prompt to use.
    systemprompt: "You are a terse assistant. You reply with short answers. You are often joyful, sometimes humorous, sometimes sarcastic."
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""

# You can remove this section. The one embedded in
# https://github.com/maruel/sillybot/blob/main/default_config.yml will be used
# automatically.
knownllms:
  # Gemma 2 family
  - repo: jartine/gemma-2-9b-it-llamafile
    packagingtype: llamafile
    basename: gemma-2-9b-it.
    upstream: google/gemma-2-9b-it
  - repo: jartine/gemma-2-27b-it-llamafile
    packagingtype: llamafile
    basename: gemma-2-27b-it.
    upstream: google/gemma-2-27b-it

  # Llama 3 family
  - repo: Mozilla/Meta-Llama-3-8B-Instruct-llamafile
    packagingtype: llamafile
    basename: Meta-Llama-3-8B-Instruct.
    upstream: meta-llama/Meta-Llama-3-8B-Instruct
  - repo: Mozilla/Meta-Llama-3-70B-Instruct-llamafile
    packagingtype: llamafile
    basename: Meta-Llama-3-70B-Instruct.
    upstream: meta-llama/Meta-Llama-3-70B-Instruct

  # Mistral family
  - repo: MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF
    packagingtype: gguf
    basename: Mistral-7B-Instruct-v0.3.
    upstream: mistralai/Mistral-7B-Instruct-v0.3
  - repo: Mozilla/Mixtral-8x7B-Instruct-v0.1-llamafile
    packagingtype: llamafile
    basename: mixtral-8x7b-instruct-v0.1
    upstream: mistralai/Mixtral-8x7B-Instruct-v0.1

  # Phi-3 family
  - repo: Mozilla/Phi-3-mini-4k-instruct-llamafile
    packagingtype: llamafile
    basename: Phi-3-mini-4k-instruct.
    upstream: microsoft/Phi-3-mini-4k-instruct
  - repo: Mozilla/Phi-3-medium-128k-instruct-llamafile
    packagingtype: llamafile
    basename: Phi-3-medium-128k-instruct.
    upstream: microsoft/Phi-3-medium-128k-instruct
