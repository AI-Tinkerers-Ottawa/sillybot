# Copyright 2024 Marc-Antoine Ruel. All rights reserved.
# Use of this source code is governed under the Apache License, Version 2.0
# that can be found in the LICENSE file.

# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Mistral-7B-Instruct-v0.3.Q5_K_M is fast enough on unaccelerated CPU and
    # takes 9GiB of RAM which should work on most computers. It has a permissive
    # license.
    # Use "python" to use the embedded pytorch generator.
    model: Mistral-7B-Instruct-v0.3.Q5_K_M
    # Default system prompt to use.
    system_prompt: "You are an AI assistant. You are the LLM model {{.Model}}. You reply with short answers. You are often joyful, sometimes humorous, sometimes sarcastic. Now is {{.Now}}."
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""

# You can remove this section. The one embedded in
# https://github.com/maruel/sillybot/blob/main/default_config.yml will be used
# automatically.
knownllms:
  # Gemma 2 family
  - repo: bartowski/gemma-2-9b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-9b-it-
    upstream: google/gemma-2-9b-it
    # https://ai.google.dev/gemma/docs/formatting
    prompt_encoding:
      begin_of_text: ""
      start_token_system: "<start_of_turn>user\n"
      end_token_system: "<end_of_turn>\n"
      start_token_user: "<start_of_turn>user\n"
      end_token_user: "<end_of_turn>\n"
      start_token_model: "<start_of_turn>model\n"
      end_token_model: "<end_of_turn>\n"
  - repo: bartowski/gemma-2-27b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-27b-it-
    upstream: google/gemma-2-27b-it
    # https://ai.google.dev/gemma/docs/formatting
    prompt_encoding:
      begin_of_text: ""
      start_token_system: "<start_of_turn>user\n"
      end_token_system: "<end_of_turn>\n"
      start_token_user: "<start_of_turn>user\n"
      end_token_user: "<end_of_turn>\n"
      start_token_model: "<start_of_turn>model\n"
      end_token_model: "<end_of_turn>\n"

  # Llama 3 family
  - repo: bartowski/Meta-Llama-3-8B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-8B-Instruct-
    upstream: meta-llama/Meta-Llama-3-8B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    prompt_encoding:
      begin_of_text: "<|begin_of_text|>"
      start_token_system: "<|start_header_id|>system<|end_header_id|>\n"
      end_token_system: "<end_of_turn>\n"
      start_token_user: "<|start_header_id|>user<|end_header_id|>\n"
      end_token_user: "<|eot_id|>\n"
      start_token_model: "<|start_header_id|>assistant<|end_header_id|>\n"
      end_token_model: "<|eot_id|>\n"
    # TODO: This repo put Q5_K_L and higher into subdirectories, which is
    # unsupported.
  - repo: bartowski/Meta-Llama-3-70B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-70B-Instruct-
    upstream: meta-llama/Meta-Llama-3-70B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    prompt_encoding:
      begin_of_text: "<|begin_of_text|>"
      start_token_system: "<|start_header_id|>system<|end_header_id|>\n"
      end_token_system: "<end_of_turn>\n"
      start_token_user: "<|start_header_id|>user<|end_header_id|>\n"
      end_token_user: "<|eot_id|>\n"
      start_token_model: "<|start_header_id|>assistant<|end_header_id|>\n"
      end_token_model: "<|eot_id|>\n"

  # Mistral family
    # bartowski/Mistral-7B-Instruct-v0.3-GGUF doesn't contain F16 which is
    # useful on a Mac.
  - repo: MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF
    packagingtype: gguf
    basename: Mistral-7B-Instruct-v0.3.
    upstream: mistralai/Mistral-7B-Instruct-v0.3
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    prompt_encoding:
      begin_of_text: "<s>"
      start_token_system: "[INST] "
      end_token_system: " [/INST]\n"
      start_token_user: "[INST] "
      end_token_user: " [/INST]\n"
      start_token_model: ""
      end_token_model: "</s>\n"
    # bartowski didn't upload this one.
  - repo: MaziyarPanahi/Mixtral-8x7B-Instruct-v0.1-GGUF
    packagingtype: gguf
    basename: Mixtral-8x7B-Instruct-v0.1.
    upstream: mistralai/Mixtral-8x7B-Instruct-v0.1
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    prompt_encoding:
      begin_of_text: "<s>"
      start_token_system: "[INST] "
      end_token_system: " [/INST]\n"
      start_token_user: "[INST] "
      end_token_user: " [/INST]\n"
      start_token_model: ""
      end_token_model: "</s>\n"

  # Phi-3 family
    # bartowski/Phi-3-mini-4k-instruct-GGUF doesn't contain F16 which is useful
    # on a Mac.
  - repo: SanctumAI/Phi-3-mini-4k-instruct-GGUF
    packagingtype: gguf
    basename: phi-3-mini-4k-instruct.
    upstream: microsoft/Phi-3-mini-4k-instruct
    # https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#chat-format
    prompt_encoding:
      begin_of_text: ""
      start_token_system: "<|system|>\n"
      end_token_system: "<|end|>\n"
      start_token_user: "<|user|>\n"
      end_token_user: "<|end|>\n"
      start_token_model: "<|assistant|>\n"
      end_token_model: "<|end|>\n"
  - repo: bartowski/Phi-3-medium-128k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3-medium-128k-instruct-
    upstream: microsoft/Phi-3-medium-128k-instruct
    # https://huggingface.co/microsoft/Phi-3-medium-128k-instruct#chat-format
    prompt_encoding:
      begin_of_text: ""
      start_token_system: "<|system|>\n"
      end_token_system: "<|end|>\n"
      start_token_user: "<|user|>\n"
      end_token_user: "<|end|>\n"
      start_token_model: "<|assistant|>\n"
      end_token_model: "<|end|>\n"
