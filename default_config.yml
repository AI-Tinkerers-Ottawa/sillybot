# Copyright 2024 Marc-Antoine Ruel. All rights reserved.
# Use of this source code is governed under the Apache License, Version 2.0
# that can be found in the LICENSE file.

# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Mistral-7B-Instruct-v0.3.Q5_K_M is fast enough on unaccelerated CPU and
    # takes 9GiB of RAM which should work on most computers. It has a permissive
    # license.
    # Use "python" to use the embedded pytorch generator.
    model: Mistral-7B-Instruct-v0.3.Q5_K_M
    # Default system prompt to use.
    system_prompt: "You are an AI assistant. You are the LLM model {{.Model}}. You reply with short answers. You are often joyful, sometimes humorous, sometimes sarcastic. Now is {{.Now}}."
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""

# You can remove this section. The one embedded in
# https://github.com/maruel/sillybot/blob/main/default_config.yml will be used
# automatically.
knownllms:
  # Gemma 2 family:
  # https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315
  - repo: bartowski/gemma-2-9b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-9b-it-
    upstream: google/gemma-2-9b-it
    # https://ai.google.dev/gemma/docs/formatting
    #prompt_encoding:
    #  begin_of_text: ""
    #  start_token_system: "<start_of_turn>user\n"
    #  end_token_system: "<end_of_turn>\n"
    #  start_token_user: "<start_of_turn>user\n"
    #  end_token_user: "<end_of_turn>\n"
    #  start_token_model: "<start_of_turn>model\n"
    #  end_token_model: "<end_of_turn>\n"
  - repo: bartowski/gemma-2-27b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-27b-it-
    upstream: google/gemma-2-27b-it
    # https://ai.google.dev/gemma/docs/formatting
    #prompt_encoding:
    #  begin_of_text: ""
    #  start_token_system: "<start_of_turn>user\n"
    #  end_token_system: "<end_of_turn>\n"
    #  start_token_user: "<start_of_turn>user\n"
    #  end_token_user: "<end_of_turn>\n"
    #  start_token_model: "<start_of_turn>model\n"
    #  end_token_model: "<end_of_turn>\n"

  # Llama 3 family:
  # https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6
  - repo: bartowski/Meta-Llama-3-8B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-8B-Instruct-
    upstream: meta-llama/Meta-Llama-3-8B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    #prompt_encoding:
    #  begin_of_text: "<|begin_of_text|>"
    #  start_token_system: "<|start_header_id|>system<|end_header_id|>\n"
    #  end_token_system: "<end_of_turn>\n"
    #  start_token_user: "<|start_header_id|>user<|end_header_id|>\n"
    #  end_token_user: "<|eot_id|>\n"
    #  start_token_model: "<|start_header_id|>assistant<|end_header_id|>\n"
    #  end_token_model: "<|eot_id|>\n"
    # TODO: This repo put Q5_K_L and higher into subdirectories, which is
    # unsupported.
  - repo: bartowski/Meta-Llama-3-70B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-70B-Instruct-
    upstream: meta-llama/Meta-Llama-3-70B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    #prompt_encoding:
    #  begin_of_text: "<|begin_of_text|>"
    #  start_token_system: "<|start_header_id|>system<|end_header_id|>\n"
    #  end_token_system: "<end_of_turn>\n"
    #  start_token_user: "<|start_header_id|>user<|end_header_id|>\n"
    #  end_token_user: "<|eot_id|>\n"
    #  start_token_model: "<|start_header_id|>assistant<|end_header_id|>\n"
    #  end_token_model: "<|eot_id|>\n"

  # Mistral family
  # https://huggingface.co/mistralai
    # bartowski/Mistral-7B-Instruct-v0.3-GGUF doesn't contain F16 which is
    # useful on a Mac.
  - repo: MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF
    packagingtype: gguf
    basename: Mistral-7B-Instruct-v0.3.
    upstream: mistralai/Mistral-7B-Instruct-v0.3
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    #prompt_encoding:
    #  begin_of_text: "<s>"
    #  start_token_system: "[INST] "
    #  end_token_system: " [/INST]\n"
    #  start_token_user: "[INST] "
    #  end_token_user: " [/INST]\n"
    #  start_token_model: ""
    #  end_token_model: "</s>\n"
    # bartowski didn't upload this one.
  - repo: MaziyarPanahi/Mixtral-8x7B-Instruct-v0.1-GGUF
    packagingtype: gguf
    basename: Mixtral-8x7B-Instruct-v0.1.
    upstream: mistralai/Mixtral-8x7B-Instruct-v0.1
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    # and
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/sentencepiece.py#L348
    # See py/mistral_test.py
    #prompt_encoding:
    #  begin_of_text: "<s>"
    #  start_token_system: "[INST]▁"
    #  end_token_system: " [/INST]\n"
    #  start_token_user: "[INST]▁"
    #  end_token_user: " [/INST]\n"
    #  start_token_model: ""
    #  end_token_model: ""

  # Phi-3 family
  # https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3
    # bartowski/Phi-3-mini-4k-instruct-GGUF doesn't contain F16 which is useful
    # on a Mac.
  - repo: SanctumAI/Phi-3-mini-4k-instruct-GGUF
    packagingtype: gguf
    basename: phi-3-mini-4k-instruct.
    upstream: microsoft/Phi-3-mini-4k-instruct
    # https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#chat-format
    #prompt_encoding:
    #  begin_of_text: ""
    #  start_token_system: "<|system|>\n"
    #  end_token_system: "<|end|>\n"
    #  start_token_user: "<|user|>\n"
    #  end_token_user: "<|end|>\n"
    #  start_token_model: "<|assistant|>\n"
    #  end_token_model: "<|end|>\n"
  - repo: bartowski/Phi-3-medium-128k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3-medium-128k-instruct-
    upstream: microsoft/Phi-3-medium-128k-instruct
    # https://huggingface.co/microsoft/Phi-3-medium-128k-instruct#chat-format
    #prompt_encoding:
    #  begin_of_text: ""
    #  start_token_system: "<|system|>\n"
    #  end_token_system: "<|end|>\n"
    #  start_token_user: "<|user|>\n"
    #  end_token_user: "<|end|>\n"
    #  start_token_model: "<|assistant|>\n"
    #  end_token_model: "<|end|>\n"

  # Qwen 2 family
  # https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f
  - repo: Qwen/Qwen2-7B-Instruct-GGUF
    packagingtype: gguf
    basename: qwen2-7b-instruct-
    upstream: Qwen/Qwen2-7B-Instruct
      # https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md#special-tokens
    #prompt_encoding:
    #  begin_of_text: ""
    #  start_token_system: "<|im_start|>\n"
    #  end_token_system: "<|im_end|>\n"
    #  start_token_user: "<|im_start|>\n"
    #  end_token_user: "<|im_end|>\n"
    #  start_token_model: ""
    #  end_token_model: "<|end_of_text|>\n"
