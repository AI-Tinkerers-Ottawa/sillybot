# Copyright 2024 Marc-Antoine Ruel. All rights reserved.
# Use of this source code is governed under the Apache License, Version 2.0
# that can be found in the LICENSE file.

# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Mistral-7B-Instruct-v0.3.Q5_K_M is fast on unaccelerated CPU and takes
    # 9GiB of RAM which should work on most computers. It has a permissive
    # license.
    # Use "python" to use the embedded pytorch generator.
    model:        "Mistral-7B-Instruct-v0.3.Q5_K_M"
    # Default system prompt to use.
    systemprompt: "You are a terse assistant. You reply with short answers. You are often joyful, sometimes humorous, sometimes sarcastic."
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""

knownllms:
  # Gemma 2 family
  - url:      "https://huggingface.co/jartine/gemma-2-9b-it-llamafile"
    type:     "llamafile"
    basename: "gemma-2-9b-it."
    upstream: "https://huggingface.co/google/gemma-2-9b-it"
    context:  8192
    native:   "BF16"
    license:  "https://ai.google.dev/gemma/terms"
  - url:      "https://huggingface.co/jartine/gemma-2-27b-it-llamafile"
    type:     "llamafile"
    basename: "gemma-2-27b-it."
    upstream: "https://huggingface.co/google/gemma-2-27b-it"
    context:  8192
    native:   "BF16"
    license:  "https://ai.google.dev/gemma/terms"

  # Llama 3 family
  - url:      "https://huggingface.co/Mozilla/Meta-Llama-3-8B-Instruct-llamafile"
    type:     "llamafile"
    basename: "Meta-Llama-3-8B-Instruct."
    upstream: "https://huggingface.co/meta-llama/Meta-Llama-3-8B-Instruct"
    context:  8192
    native:   "BF16"
    license:  "https://huggingface.co/meta-llama/Meta-Llama-3-8B/blob/main/LICENSE"
  - url:      "https://huggingface.co/Mozilla/Meta-Llama-3-70B-Instruct-llamafile"
    type:     "llamafile"
    basename: "Meta-Llama-3-70B-Instruct."
    upstream: "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct"
    context:  8192
    native:   "BF16"
    license:  "https://huggingface.co/meta-llama/Meta-Llama-3-70B-Instruct/blob/main/LICENSE"

  # Mistral family
  - url:      "https://huggingface.co/MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF"
    type:     "gguf"
    basename: "Mistral-7B-Instruct-v0.3."
    upstream: "https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.3"
    context:  32768
    native:   "BF16"
    license:  "Apache v2.0"
  - url:      "https://huggingface.co/Mozilla/Mixtral-8x7B-Instruct-v0.1-llamafile"
    type:     "llamafile"
    upstream: "https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1"
    basename: "mixtral-8x7b-instruct-v0.1"
    context:  32768
    native:   "BF16"
    license:  "Apache v2.0"

  # Phi-3 family
  - url:      "https://huggingface.co/Mozilla/Phi-3-mini-4k-instruct-llamafile"
    type:     "llamafile"
    upstream: "https://huggingface.co/microsoft/Phi-3-mini-4k-instruct"
    basename: "Phi-3-mini-4k-instruct."
    context:  4096
    native:   "BF16"
    license:  "MIT"
  - url:      "https://huggingface.co/Mozilla/Phi-3-medium-128k-instruct-llamafile"
    type:     "llamafile"
    upstream: "https://huggingface.co/microsoft/Phi-3-medium-128k-instruct"
    basename: "Phi-3-medium-128k-instruct."
    context:  131072
    native:   "BF16"
    license:  "MIT"
