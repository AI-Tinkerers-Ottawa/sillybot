# Copyright 2024 Marc-Antoine Ruel. All rights reserved.
# Use of this source code is governed under the Apache License, Version 2.0
# that can be found in the LICENSE file.

# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Mistral-7B-Instruct-v0.3.Q5_K_M is fast enough on unaccelerated CPU and
    # takes 9GiB of RAM which should work on most computers. It has a permissive
    # license.
    # Use "python" to use the embedded pytorch generator.
    model: Mistral-7B-Instruct-v0.3.Q5_K_M
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""
  settings:
    # TODO: fine-tune them all.
    # Default system prompt to use.
    prompt_system: "You are an AI assistant. You are the LLM model {{.Model}}. You reply with short answers. You are often joyful, sometimes humorous, sometimes sarcastic. Now is {{.Now}}."
    prompt_image: "You are autoregressive language model that specializes in creating perfect, outstanding prompts for generative art models like Stable Diffusion. Your job is to take user ideas, capture ALL main parts, and turn into amazing prompts. You have to capture everything from the user's prompt and then use your talent to make it amazing. You are a master of art styles, terminology, pop culture, and photography across the globe. Respond only with the new prompt. Exclude article words."
    prompt_labels: "You are autoregressive language model that specializes in creating perfect, outstanding meme text. Your job is to take user ideas, capture ALL main parts, and turn into amazing meme labels. You have to capture everything from the user's prompt and then use your talent to make it amazing filled with sarcasm. Respond only with the new meme text. Make it as succinct as possible. Use few words. Use exactly one comma. Exclude article words."


# You can remove this section. The one embedded in
# https://github.com/maruel/sillybot/blob/main/default_config.yml will be used
# automatically.
knownllms:
  # Gemma 2 family:
  # https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315
  - repo: bartowski/gemma-2-9b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-9b-it-
    upstream: google/gemma-2-9b-it
    # https://ai.google.dev/gemma/docs/formatting
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<start_of_turn>user\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<start_of_turn>user\n"
    #  user_token_end: "<end_of_turn>\n"
    #  assistant_token_start: "<start_of_turn>model\n"
    #  assistant_token_end: "<end_of_turn>\n"
  - repo: bartowski/gemma-2-27b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-27b-it-
    upstream: google/gemma-2-27b-it
    # https://ai.google.dev/gemma/docs/formatting
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<start_of_turn>user\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<start_of_turn>user\n"
    #  user_token_end: "<end_of_turn>\n"
    #  assistant_token_start: "<start_of_turn>model\n"
    #  assistant_token_end: "<end_of_turn>\n"

  # Llama 3 family:
  # https://huggingface.co/collections/meta-llama/meta-llama-3-66214712577ca38149ebb2b6
  - repo: bartowski/Meta-Llama-3-8B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-8B-Instruct-
    upstream: meta-llama/Meta-Llama-3-8B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    #prompt_encoding:
    #  begin_of_text: "<|begin_of_text|>"
    #  system_token_start: "<|start_header_id|>system<|end_header_id|>\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<|start_header_id|>user<|end_header_id|>\n"
    #  user_token_end: "<|eot_id|>\n"
    #  assistant_token_start: "<|start_header_id|>assistant<|end_header_id|>\n"
    #  assistant_token_end: "<|eot_id|>\n"
    # TODO: This repo put Q5_K_L and higher into subdirectories, which is
    # unsupported.
  - repo: bartowski/Meta-Llama-3-70B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-70B-Instruct-
    upstream: meta-llama/Meta-Llama-3-70B-Instruct
    # https://llama.meta.com/docs/model-cards-and-prompt-formats/meta-llama-3/
    #prompt_encoding:
    #  begin_of_text: "<|begin_of_text|>"
    #  system_token_start: "<|start_header_id|>system<|end_header_id|>\n"
    #  system_token_end: "<end_of_turn>\n"
    #  user_token_start: "<|start_header_id|>user<|end_header_id|>\n"
    #  user_token_end: "<|eot_id|>\n"
    #  assistant_token_start: "<|start_header_id|>assistant<|end_header_id|>\n"
    #  assistant_token_end: "<|eot_id|>\n"

  # Mistral family
  # https://huggingface.co/mistralai
  - repo: bartowski/Mistral-7B-Instruct-v0.3-GGUF
    packagingtype: gguf
    basename: Mistral-7B-Instruct-v0.3-
    upstream: mistralai/Mistral-7B-Instruct-v0.3
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    prompt_encoding:
      begin_of_text:                "<s>"
      system_token_start:           "[INST]\u2581"
      system_token_end:             " [/INST]"
      user_token_start:             "[INST]\u2581"
      user_token_end:               "[/INST]"
      assistant_token_start:        ""
      assistant_token_end:          "</s>"
      tools_available_token_start:  "[AVAILABLE_TOOLS]\u2581"
      tools_available_token_end:    "[/AVAILABLE_TOOLS]"
      tool_call_token_start:        "[TOOL_CALLS]\u2581"
      tool_call_token_end:          "</s>"
      tool_call_result_token_start: "[TOOL_RESULTS]\u2581"
      tool_call_result_token_end:   "[/TOOL_RESULTS]"
    # bartowski/Mixtral-8x22B-v0.1-GGUF only contain segmented files and I
    # didn't implement support yet.
  - repo: MaziyarPanahi/Mixtral-8x7B-Instruct-v0.1-GGUF
    packagingtype: gguf
    basename: Mixtral-8x7B-Instruct-v0.1.
    upstream: mistralai/Mixtral-8x7B-Instruct-v0.1
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/base.py#L10
    # and
    # https://github.com/mistralai/mistral-common/blob/main/src/mistral_common/tokens/tokenizers/sentencepiece.py#L348
    # See py/mistral_test.py
    #prompt_encoding:
    #  begin_of_text: "<s>"
    #  system_token_start: "[INST]▁"
    #  system_token_end: " [/INST]\n"
    #  user_token_start: "[INST]▁"
    #  user_token_end: " [/INST]\n"
    #  assistant_token_start: ""
    #  assistant_token_end: ""

  # Phi-3 family
  # https://huggingface.co/collections/microsoft/phi-3-6626e15e9585a200d2d761e3
    # bartowski/Phi-3-mini-4k-instruct-GGUF doesn't contain F16 which is useful
    # on a Mac.
  - repo: SanctumAI/Phi-3-mini-4k-instruct-GGUF
    packagingtype: gguf
    basename: phi-3-mini-4k-instruct.
    upstream: microsoft/Phi-3-mini-4k-instruct
    # https://huggingface.co/microsoft/Phi-3-mini-4k-instruct#chat-format
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<|system|>\n"
    #  system_token_end: "<|end|>\n"
    #  user_token_start: "<|user|>\n"
    #  user_token_end: "<|end|>\n"
    #  assistant_token_start: "<|assistant|>\n"
    #  assistant_token_end: "<|end|>\n"
  - repo: bartowski/Phi-3-medium-128k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3-medium-128k-instruct-
    upstream: microsoft/Phi-3-medium-128k-instruct
    # https://huggingface.co/microsoft/Phi-3-medium-128k-instruct#chat-format
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<|system|>\n"
    #  system_token_end: "<|end|>\n"
    #  user_token_start: "<|user|>\n"
    #  user_token_end: "<|end|>\n"
    #  assistant_token_start: "<|assistant|>\n"
    #  assistant_token_end: "<|end|>\n"

  # Qwen 2 family
  # https://huggingface.co/collections/Qwen/qwen2-6659360b33528ced941e557f
  - repo: Qwen/Qwen2-7B-Instruct-GGUF
    packagingtype: gguf
    basename: qwen2-7b-instruct-
    upstream: Qwen/Qwen2-7B-Instruct
      # https://github.com/QwenLM/Qwen/blob/main/tokenization_note.md#special-tokens
    #prompt_encoding:
    #  begin_of_text: ""
    #  system_token_start: "<|im_start|>\n"
    #  system_token_end: "<|im_end|>\n"
    #  user_token_start: "<|im_start|>\n"
    #  user_token_end: "<|im_end|>\n"
    #  assistant_token_start: ""
    #  assistant_token_end: "<|end_of_text|>\n"
