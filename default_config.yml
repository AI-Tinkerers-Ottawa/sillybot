# Copyright 2024 Marc-Antoine Ruel. All rights reserved.
# Use of this source code is governed under the Apache License, Version 2.0
# that can be found in the LICENSE file.

# This file uses the schema from
# https://pkg.go.dev/github.com/maruel/sillybot#Config

bot:
  llm:
    # Specify a "host:port" of an already running llama.cpp, llamafile or
    # py/llm.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Mistral-7B-Instruct-v0.3.Q5_K_M is fast enough on unaccelerated CPU and
    # takes 9GiB of RAM which should work on most computers. It has a permissive
    # license.
    # Use "python" to use the embedded pytorch generator.
    model: Mistral-7B-Instruct-v0.3.Q5_K_M
    # Default system prompt to use.
    systemprompt: "You are a terse assistant. You reply with short answers. You are often joyful, sometimes humorous, sometimes sarcastic."
  image_gen:
    # Specify a "host:port" of an already running py/image_gen.py server.
    #
    # Useful when you can't run both the LLM and the image generation models on
    # a single machine.
    #
    # See https://github.com/maruel/sillybot/blob/main/py/README.md for how
    # to run.
    remote: ""
    # Use "python" to use the embedded pytorch generator. The default SSD-1B
    # with LCM-LoRA takes about 4.6GiB of VRAM.
    model: ""

# You can remove this section. The one embedded in
# https://github.com/maruel/sillybot/blob/main/default_config.yml will be used
# automatically.
knownllms:
  # Gemma 2 family
  - repo: bartowski/gemma-2-9b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-9b-it-
    upstream: google/gemma-2-9b-it
  - repo: bartowski/gemma-2-27b-it-GGUF
    packagingtype: gguf
    basename: gemma-2-27b-it-
    upstream: google/gemma-2-27b-it

  # Llama 3 family
  - repo: bartowski/Meta-Llama-3-8B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-8B-Instruct-
    upstream: meta-llama/Meta-Llama-3-8B-Instruct
    # TODO: This repo put Q5_K_L and higher into subdirectories, which is
    # unsupported.
  - repo: bartowski/Meta-Llama-3-70B-Instruct-GGUF
    packagingtype: gguf
    basename: Meta-Llama-3-70B-Instruct-
    upstream: meta-llama/Meta-Llama-3-70B-Instruct

  # Mistral family
    # bartowski/Mistral-7B-Instruct-v0.3-GGUF doesn't contain F16 which is
    # useful on a Mac.
  - repo: MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF
    packagingtype: gguf
    basename: Mistral-7B-Instruct-v0.3.
    upstream: mistralai/Mistral-7B-Instruct-v0.3
    # bartowski didn't upload this one.
  - repo: MaziyarPanahi/Mixtral-8x7B-Instruct-v0.1-GGUF
    packagingtype: gguf
    basename: Mixtral-8x7B-Instruct-v0.1.
    upstream: mistralai/Mixtral-8x7B-Instruct-v0.1

  # Phi-3 family
    # bartowski/Phi-3-mini-4k-instruct-GGUF doesn't contain F16 which is useful
    # on a Mac.
  - repo: SanctumAI/Phi-3-mini-4k-instruct-GGUF
    packagingtype: gguf
    basename: phi-3-mini-4k-instruct.
    upstream: microsoft/Phi-3-mini-4k-instruct
  - repo: bartowski/Phi-3-medium-128k-instruct-GGUF
    packagingtype: gguf
    basename: Phi-3-medium-128k-instruct-
    upstream: microsoft/Phi-3-medium-128k-instruct
